{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "seed = 42069\n",
    "np.random.seed(seed)\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import arviz\n",
    "import pystan\n",
    "from scipy import sparse, stats\n",
    "from typing import Iterable, Union, Callable\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid\n",
    "import altair as alt\n",
    "from time import time, sleep\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Own files\n",
    "import utils \n",
    "import StanClasses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data we have is essentially a matrix, where the each row correspond to a person, and each column correspond to a movie. However, the matrix is very sparse and thus data is stored in sparse format (i.e. specified with indices and the corresponding values). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "DATA_DIR = 'ml-100k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, _, _ = utils.get_ml100k_data(DATA_DIR, subsample_top_users=200, subsample_top_items=200)\n",
    "df[['user_id', 'item_id']] -= 1\n",
    "\n",
    "# We are not going to use timestamp, therefore drop it\n",
    "df.drop('timestamp', axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user ids and item (movie) ids are essentially integer ranges, starting from and 1 to the number of users and items respectively. We don't have the all the unique ids when subsampling users and movies. It becomes problematic ... TODO: Write this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_relabler(df: pd.DataFrame, column: str):\n",
    "    uniques = pd.value_counts(df[column], sort=False).index.values\n",
    "    n_uniques = len(uniques)\n",
    "\n",
    "    # Count from 1 to conform with Stan (Stan counts indexes arrays starting at 1)\n",
    "    num2id = {num_:id_ for num_, id_ in zip(range(0, n_uniques), uniques)}\n",
    "    id2num = {id_:num_ for num_, id_ in zip(range(0, n_uniques), uniques)}\n",
    "    \n",
    "    df[column] = df[column].map(id2num)\n",
    "    return id2num, num2id\n",
    "\n",
    "df_num = df.copy()\n",
    "user2num, num2user = column_relabler(df_num, 'user_id')\n",
    "item2num, num2item = column_relabler(df_num, 'item_id')\n",
    "\n",
    "# p, q represents shape of the matrix as if it was dense\n",
    "p, q = len(user2num), len(item2num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_valtest = train_test_split(df_num, test_size=0.1, random_state=seed)\n",
    "df_val, df_test = train_test_split(df_valtest, test_size=0.5, random_state=seed)\n",
    "del df_valtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe dimensions:\n",
      "\n",
      "    df_train: (20699, 3)\n",
      "    df_val: (1150, 3)\n",
      "    df_test: (1150, 3)\n"
     ]
    }
   ],
   "source": [
    "print(f'''Dataframe dimensions:\n",
    "\n",
    "    df_train: {df_train.shape}\n",
    "    df_val: {df_val.shape}\n",
    "    df_test: {df_test.shape}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization \n",
    "Want to factorize the dense matrix $X_{n\\times m} \\approx U_{n\\times k}V_{k\\times m}$, where the subscripts denotes matrix shapes. The $k$ dimension denotes the user specified embedding dimension. We use different probabilistic models for the components. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Normal\n",
    "Not as simple as the simple model above, this is the analogous to the \"regular\" way when doing regression.\n",
    "\n",
    "$$ U_{ij} \\sim N(\\mu_u, \\sigma_u) $$\n",
    "$$ V_{ij} \\sim N(\\mu_v, \\sigma_v) $$\n",
    "$$ X_{ij}\\sim N((UV_{ij}), \\beta)$$\n",
    "$$ \\beta \\sim Gamma(a_\\beta, b_\\beta) $$\n",
    "\n",
    "User defined variables:\n",
    "$\\mu_u, \\sigma_u, \\mu_v, \\sigma_v, a_\\beta, b_\\beta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Non-negative Matrix Factorization\n",
    "\n",
    "$$ U_{ij}\\sim Gamma(a_u, b_u) $$\n",
    "$$ V_{ij}\\sim Gamma(a_v, b_v) $$\n",
    "$$ X_{ij}\\sim Normal(UV_{ij}, \\beta)$$\n",
    "$$ \\beta \\sim Gamma(a_\\beta, b_\\beta) $$\n",
    "\n",
    "User defined variables:\n",
    "$a_u, b_u, a_v, b_v, a_\\beta, b_\\beta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: ARD\n",
    "\n",
    "$$ U_{ij} \\sim N(\\mu_u, \\alpha_j) $$\n",
    "$$ V_{ij} \\sim N(\\mu_v, \\alpha_j) $$\n",
    "$$ X_{ij}\\sim N((UV)_{ij}, \\beta)$$\n",
    "$$ \\beta \\sim Gamma(a_\\beta, b_\\beta) $$\n",
    "\n",
    "$$ \\alpha_{ij} \\sim Gamma(a_\\alpha, b_\\alpha) $$\n",
    "\n",
    "User defined variables:\n",
    "$\\mu_u, \\mu_v, a_\\alpha, b_\\alpha, a_\\beta, b_\\beta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2K samples, 1 chain, 5 thin\n",
    "\n",
    "X_hat:            2min 30s, 2min 35, 2min 23s\n",
    "\n",
    "Array of vectors: 4min 21s, 4min 19s\n",
    "\n",
    "Matrix, no X_hat: 6min 14s, 6min 5s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Fitting models:   0%|          | 0/15 [00:00<?, ?model/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached StanModel\n",
      "Using cached StanModel\n",
      "Using cached StanModel\n",
      "Using cached StanModel\n",
      "Using cached StanModel\n",
      "Using cached StanModel\n",
      "Using cached StanModel\n",
      "Using cached StanModel\n",
      "Using cached StanModel\n",
      "Using cached StanModel\n",
      "Using cached StanModel\n",
      "Using cached StanModel\n",
      "Using cached StanModel\n",
      "Using cached StanModel\n",
      "Using cached StanModel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pystan:Maximum (flat) parameter count (1000) exceeded: skipping diagnostic tests for n_eff and Rhat.\n",
      "To run all diagnostics call pystan.check_hmc_diagnostics(fit)\n",
      "/usr/lib/python3.6/multiprocessing/reduction.py:51: UserWarning: Pickling fit objects is an experimental feature!\n",
      "The relevant StanModel instance must be pickled along with this fit object.\n",
      "When unpickling the StanModel must be unpickled first.\n",
      "  cls(buf, protocol).dump(obj)\n",
      "Fitting models:   7%|▋         | 1/15 [22:15<5:11:43, 1335.98s/model]WARNING:pystan:Maximum (flat) parameter count (1000) exceeded: skipping diagnostic tests for n_eff and Rhat.\n",
      "To run all diagnostics call pystan.check_hmc_diagnostics(fit)\n",
      "/usr/lib/python3.6/multiprocessing/reduction.py:51: UserWarning: Pickling fit objects is an experimental feature!\n",
      "The relevant StanModel instance must be pickled along with this fit object.\n",
      "When unpickling the StanModel must be unpickled first.\n",
      "  cls(buf, protocol).dump(obj)\n",
      "Fitting models:  13%|█▎        | 2/15 [26:25<3:38:51, 1010.14s/model]WARNING:pystan:Maximum (flat) parameter count (1000) exceeded: skipping diagnostic tests for n_eff and Rhat.\n",
      "To run all diagnostics call pystan.check_hmc_diagnostics(fit)\n",
      "/usr/lib/python3.6/multiprocessing/reduction.py:51: UserWarning: Pickling fit objects is an experimental feature!\n",
      "The relevant StanModel instance must be pickled along with this fit object.\n",
      "When unpickling the StanModel must be unpickled first.\n",
      "  cls(buf, protocol).dump(obj)\n",
      "Fitting models:  20%|██        | 3/15 [31:37<2:40:08, 800.71s/model] WARNING:pystan:Maximum (flat) parameter count (1000) exceeded: skipping diagnostic tests for n_eff and Rhat.\n",
      "To run all diagnostics call pystan.check_hmc_diagnostics(fit)\n",
      "/usr/lib/python3.6/multiprocessing/reduction.py:51: UserWarning: Pickling fit objects is an experimental feature!\n",
      "The relevant StanModel instance must be pickled along with this fit object.\n",
      "When unpickling the StanModel must be unpickled first.\n",
      "  cls(buf, protocol).dump(obj)\n",
      "Fitting models:  27%|██▋       | 4/15 [38:22<2:05:01, 681.91s/model]WARNING:pystan:Maximum (flat) parameter count (1000) exceeded: skipping diagnostic tests for n_eff and Rhat.\n",
      "To run all diagnostics call pystan.check_hmc_diagnostics(fit)\n",
      "/usr/lib/python3.6/multiprocessing/reduction.py:51: UserWarning: Pickling fit objects is an experimental feature!\n",
      "The relevant StanModel instance must be pickled along with this fit object.\n",
      "When unpickling the StanModel must be unpickled first.\n",
      "  cls(buf, protocol).dump(obj)\n",
      "Fitting models:  33%|███▎      | 5/15 [44:27<1:37:47, 586.76s/model]WARNING:pystan:Maximum (flat) parameter count (1000) exceeded: skipping diagnostic tests for n_eff and Rhat.\n",
      "To run all diagnostics call pystan.check_hmc_diagnostics(fit)\n",
      "/usr/lib/python3.6/multiprocessing/reduction.py:51: UserWarning: Pickling fit objects is an experimental feature!\n",
      "The relevant StanModel instance must be pickled along with this fit object.\n",
      "When unpickling the StanModel must be unpickled first.\n",
      "  cls(buf, protocol).dump(obj)\n",
      "Fitting models:  40%|████      | 6/15 [47:13<1:09:04, 460.50s/model]WARNING:pystan:Maximum (flat) parameter count (1000) exceeded: skipping diagnostic tests for n_eff and Rhat.\n",
      "To run all diagnostics call pystan.check_hmc_diagnostics(fit)\n",
      "/usr/lib/python3.6/multiprocessing/reduction.py:51: UserWarning: Pickling fit objects is an experimental feature!\n",
      "The relevant StanModel instance must be pickled along with this fit object.\n",
      "When unpickling the StanModel must be unpickled first.\n",
      "  cls(buf, protocol).dump(obj)\n",
      "Fitting models:  47%|████▋     | 7/15 [59:35<1:12:39, 544.98s/model]WARNING:pystan:Maximum (flat) parameter count (1000) exceeded: skipping diagnostic tests for n_eff and Rhat.\n",
      "To run all diagnostics call pystan.check_hmc_diagnostics(fit)\n",
      "/usr/lib/python3.6/multiprocessing/reduction.py:51: UserWarning: Pickling fit objects is an experimental feature!\n",
      "The relevant StanModel instance must be pickled along with this fit object.\n",
      "When unpickling the StanModel must be unpickled first.\n",
      "  cls(buf, protocol).dump(obj)\n",
      "Fitting models:  53%|█████▎    | 8/15 [1:15:30<1:17:56, 668.02s/model]WARNING:pystan:Maximum (flat) parameter count (1000) exceeded: skipping diagnostic tests for n_eff and Rhat.\n",
      "To run all diagnostics call pystan.check_hmc_diagnostics(fit)\n",
      "/usr/lib/python3.6/multiprocessing/reduction.py:51: UserWarning: Pickling fit objects is an experimental feature!\n",
      "The relevant StanModel instance must be pickled along with this fit object.\n",
      "When unpickling the StanModel must be unpickled first.\n",
      "  cls(buf, protocol).dump(obj)\n",
      "Fitting models:  60%|██████    | 9/15 [1:18:55<52:54, 529.06s/model]  WARNING:pystan:Maximum (flat) parameter count (1000) exceeded: skipping diagnostic tests for n_eff and Rhat.\n",
      "To run all diagnostics call pystan.check_hmc_diagnostics(fit)\n",
      "/usr/lib/python3.6/multiprocessing/reduction.py:51: UserWarning: Pickling fit objects is an experimental feature!\n",
      "The relevant StanModel instance must be pickled along with this fit object.\n",
      "When unpickling the StanModel must be unpickled first.\n",
      "  cls(buf, protocol).dump(obj)\n",
      "Fitting models:  67%|██████▋   | 10/15 [1:24:47<39:39, 475.93s/model]WARNING:pystan:Maximum (flat) parameter count (1000) exceeded: skipping diagnostic tests for n_eff and Rhat.\n",
      "To run all diagnostics call pystan.check_hmc_diagnostics(fit)\n",
      "WARNING:pystan:3 of 500 iterations ended with a divergence (0.6 %).\n",
      "WARNING:pystan:Try running with adapt_delta larger than 0.8 to remove the divergences.\n",
      "/usr/lib/python3.6/multiprocessing/reduction.py:51: UserWarning: Pickling fit objects is an experimental feature!\n",
      "The relevant StanModel instance must be pickled along with this fit object.\n",
      "When unpickling the StanModel must be unpickled first.\n",
      "  cls(buf, protocol).dump(obj)\n",
      "Fitting models:  73%|███████▎  | 11/15 [1:44:35<45:57, 689.49s/model]"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    StanClasses.NormalFactorizer,\n",
    "    StanClasses.NonNegativeFactorizer,\n",
    "    StanClasses.ARD_Factorizer\n",
    "]\n",
    "\n",
    "init_kwargs = {'n_components':[5,10,15,20,25]}\n",
    "static_kwargs = {'chains':1, 'iter':1000, 'control':{'max_treedepth':15}}\n",
    "\n",
    "t0 = time()\n",
    "hist = utils.fit_and_evaluate_models(\n",
    "    models=models,\n",
    "    X_train=df_train,\n",
    "    X_val=df_val,\n",
    "    candidate_kwargs=init_kwargs,\n",
    "    static_kwargs=static_kwargs\n",
    ")\n",
    "evaltime = time()-t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hist = pd.DataFrame(hist)\n",
    "# df_hist['model'] = df_hist['model'].map(lambda x: type(x).__name__)\n",
    "\n",
    "df_hist.to_pickle('histpickle_withmodels.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_hist = pd.read_pickle('histpickle_withmodels.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _plot_ci(self, n, P, lower_bounds, upper_bounds, ax, *args,\n",
    "             **kwargs):\n",
    "    '''\n",
    "    Plots credible intervals\n",
    "    '''\n",
    "    means = P.mean(axis=0)\n",
    "\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    ax.errorbar(range(n), means,\n",
    "                yerr=[means-lower_bounds, upper_bounds-means],\n",
    "                fmt='o', *args, **kwargs)\n",
    "\n",
    "def ci(self, n_elements: int=20, row_inds: Iterable=None, \n",
    "       col_inds: Iterable=None, n_samples: int=1000, p=0.95, plot: bool=False, \n",
    "       ax: 'matplotlib.Axes'=None, *args, **kwargs):\n",
    "    '''\n",
    "    Computes credible intervals first elements of matrix.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    n_elements: Number of elements to calculte credible intervals for, \n",
    "                no effect if col_inds and row_inds are given.\n",
    "    row_inds: Optional, which row indices in X to show CIs for\n",
    "\n",
    "    col_inds: Optional, which column indices in X to show CIs for\n",
    "\n",
    "    n_samples: Number of samples to sample from predictive distribution\n",
    "\n",
    "    p: Optional, credible interval percentage, 0.95 by default\n",
    "\n",
    "    plot: Optional, to plot credible intervals or not, False by default\n",
    "\n",
    "    ax: Optional, plots on given ax, no effect if show is False\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    (lower_bounds, upper_bounds)\n",
    "    '''\n",
    "    self.assert_fitted()\n",
    "\n",
    "    # This is equivalent to Xs = np.array([U@V for U,V in zip(Us, Vs)])\n",
    "    Xs = self.Us@self.Vs\n",
    "\n",
    "    if (row_inds is None) or (col_inds is None):\n",
    "        assert (row_inds and col_inds) is None,\\\n",
    "            \"Either row_inds and col_inds are both None, or both Iterables\"\n",
    "        # Used to extract first n_elements from predicted Xs\n",
    "        row_inds, col_inds = np.unravel_index(range(n_elements), Xs.shape[1:])\n",
    "\n",
    "    assert len(row_inds) == len(col_inds),\\\n",
    "        \"Length mismatch between row_inds and col_inds\"\n",
    "\n",
    "    # Sample from predictive distribution\n",
    "    picks = np.random.randint(0, len(Xs), n_samples)\n",
    "    P = Xs[picks][:,row_inds, col_inds]\n",
    "    P = self._likelihood_sample(P, picks)\n",
    "    P.sort(axis=0)\n",
    "\n",
    "    # Get credible intervals of samples from predictive distribution\n",
    "    half_p = (1-p)/2\n",
    "    lb = np.floor((half_p*n_samples)).astype(int)\n",
    "    ub = np.ceil((p+half_p)*n_samples).astype(int)\n",
    "\n",
    "    lower_bounds, upper_bounds = P[lb], P[ub]\n",
    "    \n",
    "    if plot:\n",
    "        _plot_ci(self, len(row_inds), P, lower_bounds, upper_bounds, ax, *args,\n",
    "                    **kwargs)\n",
    "\n",
    "    return lower_bounds, upper_bounds\n",
    "\n",
    "def ci_df(self, df):\n",
    "    fig, ax = plt.subplots(figsize=(10,5))\n",
    "    self.ci(row_inds=df.user_id, col_inds=df.item_id, plot=True, zorder=0, ax=ax, \n",
    "       c='firebrick', ecolor='goldenrod')\n",
    "    ax.scatter(range(len(df)), df.rating, marker='x', c='orangered', zorder=1)\n",
    "    plt.show()\n",
    "\n",
    "ci_df(hist['model'][0], df_train.sort_values('rating'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrange = np.linspace(-0.5,4,1000)\n",
    "\n",
    "a = 2\n",
    "b = 3*a\n",
    "y = stats.gamma.pdf(xrange, a=a, scale=1/b)\n",
    "print(a, b)\n",
    "plt.plot(xrange, y)\n",
    "plt.show()\n",
    "\n",
    "a = 2\n",
    "b = 4*a\n",
    "y = stats.gamma.pdf(xrange, a=a, scale=1/b)\n",
    "print(a, b)\n",
    "plt.plot(xrange, y)\n",
    "plt.show()\n",
    "\n",
    "a = 1\n",
    "b = 0.08*a\n",
    "y = stats.gamma.pdf(xrange, a=a, scale=1/b)\n",
    "print(a, b)\n",
    "plt.plot(xrange, y)\n",
    "# plt.axvline(((a-1)/b/(a-1)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_test_code = utils.get_stan_code('sanity.stan')\n",
    "sm_test = utils.StanModel_cache(sm_test_code, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_test = sm_test.sampling(algorithm=\"Fixed_param\", chains=4, n_jobs=-1, iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[0.674531,0.560879],[-1.82799,0.0132566]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A@A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
