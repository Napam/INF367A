{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pystan \n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import arviz # For visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data-generating function\n",
    "def f(x):\n",
    "    return 10*x + 15\n",
    "    \n",
    "# Returns a noisy sample from f\n",
    "def noisyF(x, sigma=0.1):\n",
    "    return np.random.normal(f(x), sigma)\n",
    "    \n",
    "# Samples n_samples independent noisy samples from f where x is selected uniformly at random from interval\n",
    "def sampleF(n_samples, interval=[0, 6], sigma=0.1):\n",
    "    x = np.random.uniform(0, 6, n_samples)\n",
    "    y = noisyF(x, sigma)\n",
    "    return x, y\n",
    "\n",
    "# Polynomial basis functions\n",
    "def phi(x, degree=1):\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    return poly.fit_transform(x.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "nr_samples = 50\n",
    "noise_sigma = 5\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "(x_train_orig, y_train) = sampleF(nr_samples, sigma=noise_sigma)\n",
    "(x_test_orig, y_test) = sampleF(nr_samples, sigma=noise_sigma)\n",
    "\n",
    "poly = PolynomialFeatures(1)\n",
    "x_train = np.matrix(poly.fit_transform(x_train_orig.reshape(-1, 1)))\n",
    "x_test = np.matrix(poly.fit_transform(x_test_orig.reshape(-1, 1)))\n",
    "y_train = np.matrix(y_train).T\n",
    "y_test = np.matrix(y_test).T\n",
    "\n",
    "plt.scatter(x_train_orig, y_train.tolist())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Linear regression\n",
    "\n",
    "Assume that we have observed $n$ pairs $(x_i, y_i)$ where $x_i$ are the feature values and $y_i$ is the label. Let $w\\in \\mathbb{R}^d$ be our regression weights. The likelihood is \n",
    "$$P(y | x, w, \\beta) = \\prod_{i=1}^{n} N(y_i | w^Tx_i, \\beta^{-1}).$$\n",
    "The parameter $\\beta$ has a Gamma prior\n",
    "$$P(\\beta) = Gamma(\\beta | a_0, b_0)$$\n",
    "where $a_0$ and $b_0$ are user-defined hyperparameters. The regression weights have a Gaussian prior\n",
    "$$P(w) = N(w | \\mathbf{0}, \\alpha^{-1}\\mathbf{I})$$\n",
    "where $\\alpha$ (prior precision) is a user-defined hyperparameter. $$$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a standard linear regression model using Stan\n",
    "lr_std_code = \"\"\"\n",
    "data {\n",
    "    int<lower=0> N;      // Number of observations\n",
    "    int<lower=0> D;      // Number of dimensions\n",
    "    vector[N] Y;         // Labels\n",
    "    matrix[N, D] X;      // Data\n",
    "    real<lower=0> a0;     // A hyperparameter for noise precision\n",
    "    real<lower=0> b0;     // A hyperparameter for noise precision\n",
    "    real<lower=0> alpha; // Prior precision\n",
    "}\n",
    "parameters {\n",
    "    real<lower=0> beta;  // Noise precision\n",
    "    vector[D] W;         // Weights\n",
    "}\n",
    "model {\n",
    "    for (n in 1:N) {\n",
    "        Y[n] ~ normal(X[n]*W, sqrt(1/beta)); // Likelihood\n",
    "    }\n",
    "    // Priors\n",
    "    beta ~ gamma(a0, b0);\n",
    "    W ~ normal(0, sqrt(1/alpha));\n",
    "}\n",
    "\"\"\" \n",
    "\n",
    "# Compile the model\n",
    "lr_std = pystan.StanModel(model_code = lr_std_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 1\n",
    "\n",
    "phi_train = phi(x_train_orig, degree=degree)\n",
    "\n",
    "n = phi_train.shape[0]\n",
    "d = phi_train.shape[1]\n",
    "\n",
    "# Hyperparameters\n",
    "alpha= 0.1\n",
    "a0 = 0.01\n",
    "b0 = 0.01\n",
    "\n",
    "# Input data for sampler\n",
    "# A dictionary: the keys and the types of items are described by the \"data\" block of your model\n",
    "data_std = {'N': n,\n",
    "           'D': d,\n",
    "           'Y': np.array(y_train).reshape(n,),\n",
    "           'X': phi_train,\n",
    "           'a0': a0,\n",
    "           'b0': b0,\n",
    "           'alpha': alpha}\n",
    "\n",
    "# Parameters that control the HMC sampler\n",
    "params = {\n",
    "    'max_treedepth': 10\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fit_std = lr_std.sampling(data=data_std, iter=2000, chains=4, control=params) # Sample from the posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a summary of inference results\n",
    "# Includes the mean, standard error, standard deviation, and various percentiles. \n",
    "# n_eff can be used to assess mixing, Rhat for assessing convergence\n",
    "# lp__ is the (unnormalized) log-posterior\n",
    "print(fit_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot posterior marginals (left) and traceplots (right) \n",
    "arviz.plot_trace(fit_std)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract samples\n",
    "fit_std.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, w, beta, n_predictions=1):\n",
    "    n_samples = w_samples.shape[0]\n",
    "    preds = []\n",
    "    for i in range(n_predictions):\n",
    "        ind = np.random.randint(0, n_samples)\n",
    "        m = np.dot(w[ind, :].reshape(-1, 1).T, x.T)\n",
    "        sample = norm.rvs(loc=m, scale=np.sqrt(1/beta[ind]))\n",
    "        preds.append(sample)\n",
    "    return np.array(preds)\n",
    "\n",
    "def credibleIntervalSamples(x, w_samples, beta_samples, ci, n_predictions=1000):\n",
    "    preds =  predict(x, w_samples, beta_samples, n_predictions)\n",
    "\n",
    "    preds_sorted = np.sort(preds)\n",
    "\n",
    "    lb_ind = int(np.ceil((1 - ci)/2*n_predictions))\n",
    "    lb = preds_sorted[lb_ind]\n",
    "\n",
    "    ub_ind = int(np.floor((ci + (1 - ci)/2)*n_predictions))\n",
    "    ub = preds_sorted[ub_ind]\n",
    "    \n",
    "    m = np.mean(preds, axis=0)\n",
    "    \n",
    "    return m, lb, ub\n",
    "\n",
    "def plotCredibleIntervals(x_grid, w_samples, beta_samples, ci=0.95, n_predictions=1000):\n",
    "    ms = []\n",
    "    lbs = []\n",
    "    ubs = []\n",
    "    for xx in x_grid:\n",
    "        m, lb, ub = credibleIntervalSamples(phi(np.array([xx]), degree=degree), w_samples, beta_samples, ci, n_predictions=n_predictions)\n",
    "        ms.append(m)\n",
    "        lbs.append(lb)\n",
    "        ubs.append(ub)\n",
    "\n",
    "    plt.scatter(x_train_orig, y_train.tolist())\n",
    "    plt.plot(x_grid, ms)\n",
    "    plt.plot(x_grid, lbs, c='r')\n",
    "    plt.plot(x_grid, ubs, c='r')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot credible intervals \n",
    "ci = 0.95\n",
    "n_predictions = 1500\n",
    "\n",
    "w_samples = fit_std.extract()['W']\n",
    "beta_samples = fit_std.extract()['beta']\n",
    "\n",
    "x_grid = np.linspace(-3, 7, 51)\n",
    "\n",
    "plotCredibleIntervals(x_grid, w_samples, beta_samples, ci=ci, n_predictions=n_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARD regression\n",
    "\n",
    "Assume that we have observed $n$ pairs $(x_i, y_i)$ where $x_i$ are the feature values and $y_i$ is the label. Let $w\\in \\mathbb{R}^d$ be our regression weights. The likelihood is \n",
    "$$P(y | x, w, \\beta) = \\prod_{i=1}^{n} N(y_i | w^Tx_i, \\beta^{-1}).$$\n",
    "\n",
    "For the noise precision $\\beta$, we use a Gamma prior\n",
    "$$ P(\\beta) = Gamma(\\beta | a_0, b_0) $$\n",
    "where $a_0$ and $b_0$ are user-defined hyperparameters. \n",
    "\n",
    "The prior for the weights $w$ is a Gaussian distribution \n",
    "$$ P(w | \\alpha_1, \\ldots \\alpha_d) = \\prod_{j=1}^{d} N(w_j | 0, \\alpha_j^{-1}). $$\n",
    "Notice that the difference compared to the standard case is that instead of having one precision parameter $\\alpha$ that is same for all dimensions, we have a separate prior precision $\\alpha_j$ for each dimension. Furthermore, $\\alpha_j$ is not assumed to be known but we place a prior on it. The prior for $\\alpha_j$ is a Gamma distribution\n",
    "$$ P(\\alpha_j) =  Gamma(\\alpha_j | c_0, d_0)\\quad \\forall j=1,\\ldots, d$$\n",
    "where $c_0$ and $d_0$ are user-defined hyperparameters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_ard_code = \"\"\"\n",
    "data {\n",
    "    int<lower=0> N;   // Number of observations\n",
    "    int<lower=0> D;   // Number of dimensions\n",
    "    matrix[N,D] X;    // Data\n",
    "    vector[N] Y;      // Labels\n",
    "    real<lower=0> a0; // A hyperparameter for noise precision\n",
    "    real<lower=0> b0; // A hyperparameter for noise precision\n",
    "    real<lower=0> c0; // A hyperparameter for prior precision (ARD)\n",
    "    real<lower=0> d0; // A hyperparameter for prior precision (ARD)\n",
    "    }    \n",
    "parameters {\n",
    "    vector[D] W;\n",
    "    vector<lower=0>[D] alpha; // Prior precisions\n",
    "    real<lower=0> beta; // Noise precision\n",
    "    }\n",
    "transformed parameters {\n",
    "    vector<lower=0>[D] t_alpha; // Prior variances\n",
    "    real<lower=0> t_beta; // Noise variance\n",
    "    for (d in 1:D) {\n",
    "        t_alpha[d] = 1/sqrt(alpha[d]);\n",
    "        }\n",
    "    t_beta =  1/sqrt(beta);\n",
    "    }\n",
    "model {\n",
    "    beta ~ gamma(a0, b0);\n",
    "    alpha ~ gamma(c0, d0);\n",
    "    W ~ normal(0,  t_alpha);\n",
    "    Y ~ normal(X*W, t_beta);\n",
    "    }\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_ard = pystan.StanModel(model_code = lr_ard_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 1\n",
    "\n",
    "phi_train = phi(x_train_orig, degree=degree)\n",
    "\n",
    "n = phi_train.shape[0]\n",
    "d = phi_train.shape[1]\n",
    "\n",
    "# Hyperparameters\n",
    "a0 = 0.01\n",
    "b0 = 0.01\n",
    "c0 = 0.01\n",
    "d0 = 0.01\n",
    "\n",
    "# Input data for sampler\n",
    "# A dictionary: the keys and the types of items are described by the \"data\" block of your model\n",
    "data_ard = {'N': n,\n",
    "           'D': d,\n",
    "           'Y': np.array(y_train).reshape(n,),\n",
    "           'X': phi_train,\n",
    "           'a0': a0,\n",
    "           'b0': b0,\n",
    "           'c0': c0,\n",
    "           'd0': d0}\n",
    "\n",
    "# Parameters that control the HMC sampler\n",
    "params = {\n",
    "    'max_treedepth': 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fit_ard = lr_ard.sampling(data=data_ard, iter=2000, chains=4, control=params) # Sample from the posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fit_ard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot posterior marginals (left) and traceplots (right) \n",
    "arviz.plot_trace(fit_ard)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot credible intervals \n",
    "ci = 0.95\n",
    "n_predictions = 1000\n",
    "\n",
    "w_samples = fit_ard.extract()['W']\n",
    "beta_samples = fit_ard.extract()['beta']\n",
    "\n",
    "x_grid = np.linspace(-3, 7, 51)\n",
    "\n",
    "plotCredibleIntervals(x_grid, w_samples, beta_samples, ci=ci, n_predictions=n_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stan2",
   "language": "python",
   "name": "stan2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
